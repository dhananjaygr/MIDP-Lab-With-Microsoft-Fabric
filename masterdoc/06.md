### Exercise 4: Glimpse of Purview to govern the overall data and analytics estate. <a name="tee-up-the-purview"></a>

In Exercise 1, you loaded raw data into the Lakehouse. Then, in Exercise 2, you used a Delta Live Table pipeline to transform it into a data product for downstream consumption by analysts. 

In Exercise 3, you applied machine learning operations on this data product to build a customer sentiment model. This sentiment analysis allows Wide World Importers to determine which hashtags are trending so they can customize their campaigns to improve their sales while retaining their existing customers.

Meanwhile, Microsoft Purview provides a unified data governance service that helps manage and govern Wide World Importers’ data, which is stored in multi-cloud environments and in data sources such as Oracle, Teradata, ADLS Gen2, and Azure SQL Database.

In this exercise, you will explore Wide World Importers data estate that’s registered in Microsoft Purview.

1.	In the Azure portal web session (tab), in the search box (located across the top of the page), enter:**Microsoft Purview**.

2.	In the search results pane, select **Microsoft Purview accounts**.

![Search Microsoft Purview](https://github.com/SD-14/Ignite-Demo/blob/main/media/images/img402.png?raw=true)

3.	In the **Microsoft Purview accounts** page, select the resource that has a name starting with **purviewanalytics**.

>**Note:** Each user has their own unique instance of this resource.


![Select the resource](https://github.com/SD-14/Ignite-Demo/blob/main/media/images/img403.png?raw=true)

4.	In the Microsoft Purview accounts resource page, in the **Open Microsoft Purview Governance Portal** tile, select the **Open** link.

![Open Microsoft Purview](https://github.com/SD-14/Ignite-Demo/blob/main/media/images/image4004.png?raw=true)

*Microsoft Purview Governance Portal opens in a new web session (tab).*

5.	In the Microsoft Purview Governance Portal web session (tab), select the [**Documentation** link](https://docs.microsoft.com/en-us/azure/purview/use-azure-purview-studio)

![select the documentation link](https://github.com/SD-14/Ignite-Demo/blob/main/media/images/image4005.png?raw=true)

6.	Review the documentation available to support you setting up and working with Microsoft Purview.

*The Microsoft Purview documentation can help you learn how to:*

- Create a holistic, up-to-date map of your data landscape with automated data discovery, sensitive data classification, and end-to-end data lineage.
- Enable data curators to manage and secure your data estate.
- Empower data consumers to find valuable, trustworthy data.

7.	In the Microsoft Purview Governance Portal web session (tab), at the left, select the **Data map** icon (the second icon from the top).

*Data map makes your data meaningful by graphing your data assets and their relationships across your data estate. Use data map to discover data and manage access to that data.*

8.	In the left pane, select **Sources**.

![Select Sources](https://github.com/SD-14/Ignite-Demo/blob/main/media/images/image4008.png?raw=true)

9. In the map view, for the root collection item, select the plus (+) icon to reveal the collections.

![Select the + icon](https://github.com/SD-14/Ignite-Demo/blob/main/media/images/image4009.png?raw=true)

10.	Expand each of the collections to review specific sources related to those collections.

![Sources related to the collection](https://github.com/SD-14/Ignite-Demo/blob/main/media/images/image4010.png?raw=true)

----

Congratulations! You as Data Engineers, have helped Wide World Importers gain actionable insights from its disparate data sources, thereby contributing to future growth, customer satisfaction, and competitive advantage.

In this lab we experienced the creation of a simple, integrated, open and governed Data Lakehouse foundation using the Microsoft Analytics Solution Pattern. 

In this lab we covered the following: 
1.	As a first step we looked at data ingestion from a spectrum of analytical and operational data sources into the Lakehouse. We started with streaming data and analytics pipeline using ADX for a near real-time analytics scenario, followed by Synapse pipelines that ingested raw data from analytical/operational data sources to the Bronze layer. 

2.	The second step was to explore offline data and analytics pipeline using open Delta format and Azure Databricks Delta Live Tables. We stitched streaming and non-streaming data (landed earlier) together, to create a combined data product to build a simple Lakehouse.

3.	In the third step, we explored ML and BI scenarios on the Lakehouse. Here we reviewed MLOps pipeline using the Azure Databricks managed MLflow with Azure ML. Then, using Power BI with Synapse serverless SQL pool capabilities, we derived actionable insights. We explored SQL Analytics with Azure Databricks and Azure Synapse Serverless. Last, but not the least, we leveraged Purview for data governance. 
